#!/usr/bin/env ruby
$LOAD_PATH.unshift File.expand_path('../../lib', __FILE__)
require 'rubygems'
require 'cat_esri'
require 'trollop'

crawler = CatEsri::Crawler.new(STDOUT)
ARGV << "-h" if ARGV.size == 0 #force help message if no args

opts = Trollop::options do
  version "LogicalCat: cat_esri #{CatEsri::VERSION} (c) 2012 LogicalCat LLC"
  banner <<-EOS
o-----------------------------------------------------------------------------o
| Collect text content and file attributes from ESRI shapefiles, Personal     |
| Geodatabases (windows only) and File Geodatabases (bit of a hack)           |
o-----------------------------------------------------------------------------o
|                                                                             |
|                         LogicalCat ESRI crawler                             |
|                                                                             |
|          Usage: cat_esri -p <path> -o <outdir> [other options]              |
|                                                                             |
| Examples:                                                                   |
|                                                                             |
| Crawl path, write results to .csv file at c:\\temp:                          |
|  cat_esri -p P:\\geodata\\maps -f csv -o C:\\temp                              |
|                                                                             |
| Same crawl with label 'mymaps':                                             |
|  cat_esri -p P:\\geodata\\maps -f csv -o C:\\temp -n mymaps                    |
|                                                                             |
| Include ESRI geodatabases; export to sqlite3 format:                        |
|  cat_esri -p P:\\geodata\\maps -f sqlite3 -o C:\\temp -e                       |
|                                                                             |
| Crawl path for shapefiles, including GeoGraphix layers (normally skipped):  |
|  cat_esri -p P:\\geodata\\maps -g -o c:\\temp                                  |
|                                                                             |
| Use -q to exclude subdirectories from a crawl:                              |
|  cat_esri -p P:\\data -q "P:\\data\\skip_me?P:\\data\\me_too" -o c:\\temp         |
|  (separate multiple paths with a '?' character)                             |
|                                                                             |
|                                                                             |
| NOTE: Output file names are auto-generated with a type, time stamp and      |
| suffix based on format. For example, a csv output might look like this:     |
|                                                                             |
|    "c:\\temp\\MAP_1392987864628876.csv"                                       |
|                                                                             |
o-----------------------------------------------------------------------------o
EOS

  opt :path, "Network path to scan", :type => String
  opt :logfile, "Log crawl activity to file", :type => String
  opt :timeout, "Timeout for some long-running tasks", :default => 10
  opt :format, "Output format: csv or sqlite3", :type => String, :default => 'sqlite3'
  opt :xitems, "Maximum items per outfile", :default => 50000
  opt :outdir, "Write crawl results to directory", :type => String
  opt :esrigdb, "Include ESRI personal and file geodatabases", :default => false
  opt :ggxlayer, "Include GeoAtlas Layers in shapefile scans", :default => false
  opt :label, "Name used as label for this crawl", :short => 'n', :type => String
  opt :quell, "Exclude crawl subdirectories", :type => String
  opt :cloud_bucket_name, "Amazon S3 bucket name", :type => String, :short => 'B'
  opt :cloud_encryption_key, "Encryption/decryption key", :type => String, :short => 'E'
  opt :cloud_access_key_id, "Amazon S3 access key id", :type => String, :short => 'I'
  opt :cloud_secret_access_key, "Amazon S3 secret access key", :type => String, :short => 'S'
end

Trollop::die :path, "Cannot be blank" if opts[:path] == nil
Trollop::die :path, "Path must exist" unless File.exist?(opts[:path])
Trollop::die :format, "Unknown format type: '#{opts[:format]}'" unless CatEsri::FORMATS.include?(opts[:format])

opts[:path] = File.expand_path(opts[:path]) if opts[:path]
opts[:logfile] = File.expand_path(opts[:logfile]) if opts[:logfile]
opts[:outfile] = File.expand_path(opts[:outfile]) if opts[:outfile]

if opts[:outdir] == nil
  Trollop::die :cloud_bucket_name, "Amazon S3 bucket name must exist" if opts[:cloud_bucket_name] == nil
  Trollop::die :cloud_encryption_key, "Encryption/decryption key must exist" if opts[:cloud_encryption_key] == nil
  Trollop::die :cloud_access_key_id, "Amazon S3 access key id must exist" if opts[:cloud_access_key_id] == nil
  Trollop::die :cloud_secret_access_key, "Amazon S3 secret access key must exist" if opts[:cloud_secret_access_key] == nil
  opts[:format] = 'cloud'
  require 'tmpdir'
  opts[:outdir] = Dir.tmpdir
else
  Trollop::die :outdir, "Directory must exist" unless File.exists?(opts[:outdir])
end

if opts[:logfile] =~ /\\/ || opts[:logfile] =~ /\//
  Trollop::die :logfile, "Invalid logfile path." unless File.exist?(File.dirname(File.expand_path(opts[:logfile])))
end

crawler.options = opts
crawler.scan
